{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\USER\\\\Projects\\\\SourceCodeAnalysis\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"text_repo/\"\n",
    "repo = Repo.clone_from(\"https://github.com/Priya-Rathor/InterviewQuesCreatorAPP\",to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path, glob=\"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from fastapi import FastAPI, Form, Request, Response, File, Depends, HTTPException, status\\nfrom fastapi.responses import RedirectResponse\\nfrom fastapi.staticfiles import StaticFiles\\nfrom fastapi.templating import Jinja2Templates\\nfrom fastapi.encoders import jsonable_encoder\\nimport uvicorn\\nimport os\\nimport aiofiles\\nimport json\\nimport csv\\nfrom src.helper import llm_pipeline\\n\\n\\napp = FastAPI()\\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\\n\\ntemplates = Jinja2Templates(directory=\"templates\")\\n\\n\\n@app.get(\"/\")\\nasync def index(request: Request):\\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\\n\\n\\n@app.post(\"/upload\")\\nasync def chat(request: Request, pdf_file: bytes = File(), filename: str = Form(...)):\\n    base_folder = \\'static/docs/\\'\\n    if not os.path.isdir(base_folder):\\n        os.mkdir(base_folder)\\n    pdf_filename = os.path.join(base_folder, filename)\\n\\n    async with aiofiles.open(pdf_filename, \\'wb\\') as f:\\n        await f.write(pdf_file)\\n \\n    response_data = jsonable_encoder(json.dumps({\"msg\": \\'success\\',\"pdf_filename\": pdf_filename}))\\n    res = Response(response_data)\\n    return res\\n\\n\\ndef get_csv(file_path):\\n    answer_generation_chain, ques_list = llm_pipeline(file_path)\\n    base_folder = \\'static/output/\\'\\n    if not os.path.isdir(base_folder):\\n        os.mkdir(base_folder)\\n    output_file = base_folder+\"QA.csv\"\\n    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\\n        csv_writer = csv.writer(csvfile)\\n        csv_writer.writerow([\"Question\", \"Answer\"])  # Writing the header row\\n\\n        for question in ques_list:\\n            print(\"Question: \", question)\\n            answer = answer_generation_chain.run(question)\\n            print(\"Answer: \", answer)\\n            print(\"--------------------------------------------------\\\\n\\\\n\")\\n\\n            # Save answer to CSV file\\n            csv_writer.writerow([question, answer])\\n    return output_file\\n\\n\\n\\n\\n@app.post(\"/analyze\")\\nasync def chat(request: Request, pdf_filename: str = Form(...)):\\n    output_file = get_csv(pdf_filename)\\n    response_data = jsonable_encoder(json.dumps({\"output_file\": output_file}))\\n    res = Response(response_data)\\n    return res\\n\\n\\n\\nif __name__ == \"__main__\":\\n    uvicorn.run(\"app:app\", host=\\'0.0.0.0\\', port=8080, reload=True)', metadata={'source': 'text_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name = \"Generative AI Project\",\\n    version= \\'0.0.0\\',\\n    author=\"priya Rathor\",\\n    author_email=\"rathorpriya1718gmail.com\",\\n    packages= find_packages(),\\n    install_requires = []\\n)', metadata={'source': 'text_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.embeddings import OpenAIEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains.summarize import load_summarize_chain\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.chains import RetrievalQA\\nimport os\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nfrom langchain.text_splitter import TokenTextSplitter\\nfrom langchain.schema import Document\\n\\n\\n# OpenAI authentication\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n\\n\\n\\ndef file_processing(file_path):\\n\\n    # Load data from PDF\\n    loader = PyPDFLoader(file_path)\\n    data = loader.load()\\n\\n    question_gen = \\'\\'\\n\\n    for page in data:\\n        question_gen += page.page_content\\n        \\n    splitter_ques_gen = TokenTextSplitter(\\n        model_name = \\'gpt-3.5-turbo\\',\\n        chunk_size = 10000,\\n        chunk_overlap = 200\\n    )\\n\\n    chunks_ques_gen = splitter_ques_gen.split_text(question_gen)\\n\\n    document_ques_gen = [Document(page_content=t) for t in chunks_ques_gen]\\n\\n    splitter_ans_gen = TokenTextSplitter(\\n        model_name = \\'gpt-3.5-turbo\\',\\n        chunk_size = 1000,\\n        chunk_overlap = 100\\n    )\\n\\n\\n    document_answer_gen = splitter_ans_gen.split_documents(\\n        document_ques_gen\\n    )\\n\\n    return document_ques_gen, document_answer_gen\\n\\n\\n\\ndef llm_pipeline(file_path):\\n\\n    document_ques_gen, document_answer_gen = file_processing(file_path)\\n\\n    llm_ques_gen_pipeline = ChatOpenAI(\\n        temperature = 0.3,\\n        model = \"gpt-3.5-turbo\"\\n    )\\n\\n   \\n\\n    PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\n\\n    \\n\\n    REFINE_PROMPT_QUESTIONS = PromptTemplate(\\n        input_variables=[\"existing_answer\", \"text\"],\\n        template=refine_template,\\n    )\\n\\n    ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline, \\n                                            chain_type = \"refine\", \\n                                            verbose = True, \\n                                            question_prompt=PROMPT_QUESTIONS, \\n                                            refine_prompt=REFINE_PROMPT_QUESTIONS)\\n\\n    ques = ques_gen_chain.run(document_ques_gen)\\n\\n    embeddings = OpenAIEmbeddings()\\n\\n    vector_store = FAISS.from_documents(document_answer_gen, embeddings)\\n\\n    llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\\n\\n    ques_list = ques.split(\"\\\\n\")\\n    filtered_ques_list = [element for element in ques_list if element.endswith(\\'?\\') or element.endswith(\\'.\\')]\\n\\n    answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, \\n                                                chain_type=\"stuff\", \\n                                                retriever=vector_store.as_retriever())\\n\\n    return answer_generation_chain, filtered_ques_list\\n\\n\\n', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='\\nprompt_template = \"\"\"\\n    You are an expert at creating questions based on coding materials and documentation.\\n    Your goal is to prepare a coder or programmer for their exam and coding tests.\\n    You do this by asking questions about the text below:\\n\\n    ------------\\n    {text}\\n    ------------\\n\\n    Create questions that will prepare the coders or programmers for their tests.\\n    Make sure not to lose any important information.\\n\\n    QUESTIONS:\\n    \"\"\"\\n\\n\\nrefine_template = (\"\"\"\\n    You are an expert at creating practice questions based on coding material and documentation.\\n    Your goal is to help a coder or programmer prepare for a coding test.\\n    We have received some practice questions to a certain extent: {existing_answer}.\\n    We have the option to refine the existing questions or add new ones.\\n    (only if necessary) with some more context below.\\n    ------------\\n    {text}\\n    ------------\\n\\n    Given the new context, refine the original questions in English.\\n    If the context is not helpful, please provide the original questions.\\n    QUESTIONS:\\n    \"\"\"\\n    )', metadata={'source': 'text_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'text_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='from langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.embeddings import OpenAIEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains.summarize import load_summarize_chain\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.chains import RetrievalQA\\nimport os\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nfrom langchain.text_splitter import TokenTextSplitter\\nfrom langchain.schema import Document\\n\\n\\n# OpenAI authentication\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n\\n\\n\\ndef file_processing(file_path):\\n\\n    # Load data from PDF\\n    loader = PyPDFLoader(file_path)\\n    data = loader.load()\\n\\n    question_gen = \\'\\'\\n\\n    for page in data:\\n        question_gen += page.page_content\\n        \\n    splitter_ques_gen = TokenTextSplitter(\\n        model_name = \\'gpt-3.5-turbo\\',\\n        chunk_size = 10000,\\n        chunk_overlap = 200\\n    )\\n\\n    chunks_ques_gen = splitter_ques_gen.split_text(question_gen)\\n\\n    document_ques_gen = [Document(page_content=t) for t in chunks_ques_gen]\\n\\n    splitter_ans_gen = TokenTextSplitter(\\n        model_name = \\'gpt-3.5-turbo\\',\\n        chunk_size = 1000,\\n        chunk_overlap = 100\\n    )\\n\\n\\n    document_answer_gen = splitter_ans_gen.split_documents(\\n        document_ques_gen\\n    )\\n\\n    return document_ques_gen, document_answer_gen\\n\\n\\n\\ndef llm_pipeline(file_path):\\n\\n    document_ques_gen, document_answer_gen = file_processing(file_path)\\n\\n    llm_ques_gen_pipeline = ChatOpenAI(\\n        temperature = 0.3,\\n        model = \"gpt-3.5-turbo\"\\n    )\\n\\n   \\n\\n    PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\n\\n    \\n\\n    REFINE_PROMPT_QUESTIONS = PromptTemplate(\\n        input_variables=[\"existing_answer\", \"text\"],\\n        template=refine_template,\\n    )\\n\\n    ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline, \\n                                            chain_type = \"refine\", \\n                                            verbose = True, \\n                                            question_prompt=PROMPT_QUESTIONS, \\n                                            refine_prompt=REFINE_PROMPT_QUESTIONS)\\n\\n    ques = ques_gen_chain.run(document_ques_gen)\\n\\n    embeddings = OpenAIEmbeddings()\\n\\n    vector_store = FAISS.from_documents(document_answer_gen, embeddings)\\n\\n    llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\\n\\n    ques_list = ques.split(\"\\\\n\")\\n    filtered_ques_list = [element for element in ques_list if element.endswith(\\'?\\') or element.endswith(\\'.\\')]\\n\\n    answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, \\n                                                chain_type=\"stuff\", \\n                                                retriever=vector_store.as_retriever())\\n\\n    return answer_generation_chain, filtered_ques_list\\n\\n\\n', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language= Language.PYTHON,\n",
    "                                             chunk_size =500,\n",
    "                                             chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.text_splitter.RecursiveCharacterTextSplitter at 0x180f791f6a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from fastapi import FastAPI, Form, Request, Response, File, Depends, HTTPException, status\\nfrom fastapi.responses import RedirectResponse\\nfrom fastapi.staticfiles import StaticFiles\\nfrom fastapi.templating import Jinja2Templates\\nfrom fastapi.encoders import jsonable_encoder\\nimport uvicorn\\nimport os\\nimport aiofiles\\nimport json\\nimport csv\\nfrom src.helper import llm_pipeline\\n\\n\\napp = FastAPI()\\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")', metadata={'source': 'text_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='templates = Jinja2Templates(directory=\"templates\")\\n\\n\\n@app.get(\"/\")\\nasync def index(request: Request):\\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\\n\\n\\n@app.post(\"/upload\")\\nasync def chat(request: Request, pdf_file: bytes = File(), filename: str = Form(...)):\\n    base_folder = \\'static/docs/\\'\\n    if not os.path.isdir(base_folder):\\n        os.mkdir(base_folder)\\n    pdf_filename = os.path.join(base_folder, filename)', metadata={'source': 'text_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='async with aiofiles.open(pdf_filename, \\'wb\\') as f:\\n        await f.write(pdf_file)\\n \\n    response_data = jsonable_encoder(json.dumps({\"msg\": \\'success\\',\"pdf_filename\": pdf_filename}))\\n    res = Response(response_data)\\n    return res', metadata={'source': 'text_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def get_csv(file_path):\\n    answer_generation_chain, ques_list = llm_pipeline(file_path)\\n    base_folder = \\'static/output/\\'\\n    if not os.path.isdir(base_folder):\\n        os.mkdir(base_folder)\\n    output_file = base_folder+\"QA.csv\"\\n    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\\n        csv_writer = csv.writer(csvfile)\\n        csv_writer.writerow([\"Question\", \"Answer\"])  # Writing the header row', metadata={'source': 'text_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='for question in ques_list:\\n            print(\"Question: \", question)\\n            answer = answer_generation_chain.run(question)\\n            print(\"Answer: \", answer)\\n            print(\"--------------------------------------------------\\\\n\\\\n\")\\n\\n            # Save answer to CSV file\\n            csv_writer.writerow([question, answer])\\n    return output_file', metadata={'source': 'text_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='@app.post(\"/analyze\")\\nasync def chat(request: Request, pdf_filename: str = Form(...)):\\n    output_file = get_csv(pdf_filename)\\n    response_data = jsonable_encoder(json.dumps({\"output_file\": output_file}))\\n    res = Response(response_data)\\n    return res\\n\\n\\n\\nif __name__ == \"__main__\":\\n    uvicorn.run(\"app:app\", host=\\'0.0.0.0\\', port=8080, reload=True)', metadata={'source': 'text_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name = \"Generative AI Project\",\\n    version= \\'0.0.0\\',\\n    author=\"priya Rathor\",\\n    author_email=\"rathorpriya1718gmail.com\",\\n    packages= find_packages(),\\n    install_requires = []\\n)', metadata={'source': 'text_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_community.embeddings import OpenAIEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains.summarize import load_summarize_chain\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.chains import RetrievalQA\\nimport os', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nfrom langchain.text_splitter import TokenTextSplitter\\nfrom langchain.schema import Document', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# OpenAI authentication\\nload_dotenv()\\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"def file_processing(file_path):\\n\\n    # Load data from PDF\\n    loader = PyPDFLoader(file_path)\\n    data = loader.load()\\n\\n    question_gen = ''\\n\\n    for page in data:\\n        question_gen += page.page_content\\n        \\n    splitter_ques_gen = TokenTextSplitter(\\n        model_name = 'gpt-3.5-turbo',\\n        chunk_size = 10000,\\n        chunk_overlap = 200\\n    )\\n\\n    chunks_ques_gen = splitter_ques_gen.split_text(question_gen)\", metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"document_ques_gen = [Document(page_content=t) for t in chunks_ques_gen]\\n\\n    splitter_ans_gen = TokenTextSplitter(\\n        model_name = 'gpt-3.5-turbo',\\n        chunk_size = 1000,\\n        chunk_overlap = 100\\n    )\\n\\n\\n    document_answer_gen = splitter_ans_gen.split_documents(\\n        document_ques_gen\\n    )\\n\\n    return document_ques_gen, document_answer_gen\", metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def llm_pipeline(file_path):\\n\\n    document_ques_gen, document_answer_gen = file_processing(file_path)\\n\\n    llm_ques_gen_pipeline = ChatOpenAI(\\n        temperature = 0.3,\\n        model = \"gpt-3.5-turbo\"\\n    )\\n\\n   \\n\\n    PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\n\\n    \\n\\n    REFINE_PROMPT_QUESTIONS = PromptTemplate(\\n        input_variables=[\"existing_answer\", \"text\"],\\n        template=refine_template,\\n    )', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline, \\n                                            chain_type = \"refine\", \\n                                            verbose = True, \\n                                            question_prompt=PROMPT_QUESTIONS, \\n                                            refine_prompt=REFINE_PROMPT_QUESTIONS)\\n\\n    ques = ques_gen_chain.run(document_ques_gen)\\n\\n    embeddings = OpenAIEmbeddings()', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='vector_store = FAISS.from_documents(document_answer_gen, embeddings)\\n\\n    llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\\n\\n    ques_list = ques.split(\"\\\\n\")\\n    filtered_ques_list = [element for element in ques_list if element.endswith(\\'?\\') or element.endswith(\\'.\\')]', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, \\n                                                chain_type=\"stuff\", \\n                                                retriever=vector_store.as_retriever())\\n\\n    return answer_generation_chain, filtered_ques_list', metadata={'source': 'text_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='prompt_template = \"\"\"\\n    You are an expert at creating questions based on coding materials and documentation.\\n    Your goal is to prepare a coder or programmer for their exam and coding tests.\\n    You do this by asking questions about the text below:\\n\\n    ------------\\n    {text}\\n    ------------\\n\\n    Create questions that will prepare the coders or programmers for their tests.\\n    Make sure not to lose any important information.\\n\\n    QUESTIONS:\\n    \"\"\"', metadata={'source': 'text_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='refine_template = (\"\"\"\\n    You are an expert at creating practice questions based on coding material and documentation.\\n    Your goal is to help a coder or programmer prepare for a coding test.\\n    We have received some practice questions to a certain extent: {existing_answer}.\\n    We have the option to refine the existing questions or add new ones.\\n    (only if necessary) with some more context below.\\n    ------------\\n    {text}\\n    ------------', metadata={'source': 'text_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='Given the new context, refine the original questions in English.\\n    If the context is not helpful, please provide the original questions.\\n    QUESTIONS:\\n    \"\"\"\\n    )', metadata={'source': 'text_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(texts,embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(texts, embedding=embedding)\n",
    "vectordb.save_local(\"./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm = llm, memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm,retriever=vectordb.as_retriever(searxh_type=\"mmr\",search_kwargs={\"k\":8}),memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Uvicon function? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uvicorn is a lightning-fast ASGI server implementation, using uvloop and httptools. It runs asynchronous Python web code in a single process.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `app.py` file defines a FastAPI application that serves two endpoints: `/analyze` and `/upload`. \n",
      "\n",
      "The `/analyze` endpoint is a POST request that takes a `pdf_filename` parameter from a form and calls the `get_csv` function to get an output file. The response includes the output file in JSON format.\n",
      "\n",
      "The `/upload` endpoint is a POST request that takes a PDF file and a filename parameter from a form. It saves the uploaded PDF file to a specific folder in the project directory.\n",
      "\n",
      "Additionally, the file sets up a FastAPI application with endpoints for serving static files and rendering HTML templates using Jinja2Templates. The application is then run using uvicorn with the specified host and port.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is app.py file do?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
